# Comparative-Analysis-for-Aspect-Based-Sentiment-Analysis-ABSA-

## Overview
This repository contains the official code, documentation, and implementation for the research paper titled "Comparative Analysis of Large Language Models (LLMs) and Traditional Models for Aspect-Based Sentiment Analysis (ABSA)." The study evaluates and contrasts the performance of LLMs like Flan-T5 and DeBERTa with traditional models like LSTM in the context of ABSA. It aims to provide insights into the effectiveness of different model architectures in analyzing sentiment at a more granular aspect-based level.

## Table of Contents
1. [Datasets](#Datasets)
2. [Code and Results](#Code-and-Results)
3. [Performance Analysis Results](#Performance-Analysis-Results)
4. [Reference/Citation](#Reference)


## Datasets
The `datasets` directory includes datasets used in the analysis. Detailed information about data sources and preprocessing steps can be found [here](datasets).

## Code and Results
The `code and results` directory contains scripts for data preprocessing, model training, Model predictions, and visualization.Visit these links for details:

- [LLM_codes](Codes%20and%20results_LLM)
- [LSTM_codes](Codes%20and%20results_LSTM)
- [Flan-t5_codes](Codes%20and%20results_flan_t5)
- [DeBERTa_codes](codes%20and%20results_deberta) for details.

## Performance Analysis Results
Explore the `Performance Analysis Results` directory for key metrics, graphs, and visualizations obtained from the analysis. More details on the performance analysis can be found [here](Results%and%performance%analysis).

## Reference

If you use this code, please cite our paper:
Mughal, Nimra, et al. "Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis." IEEE Access (2024).
DOI/Link https://doi.org/10.1109/ACCESS.2024.3386969



## Acknowledgements
This work was supported by the National Research Program for Universities (NRPU), Higher Education Commission, Pakistan under Project Ref No. 20-14457/NRPU/R\&D/HEC/2021-2020. 

---

